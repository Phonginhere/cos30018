{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 17:43:12.818794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-06 17:43:12.819706: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-06 17:43:12.820617: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-06 17:43:12.896316: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-06 17:43:12.896867: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-06 17:43:12.897402: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 15:14:08.955898: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-08-14 15:14:09.046085: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 15:14:09.046573: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 15:14:09.047174: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-14 15:14:09.108378: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 15:14:09.108821: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 15:14:09.109223: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-14 15:14:09.361816: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 15:14:09.362573: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 15:14:09.362999: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-14 15:14:09.424066: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 15:14:09.424486: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 15:14:09.424950: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 15:14:23.901273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 15:14:23.901938: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 15:14:23.902414: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-14 15:14:23.963766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 15:14:23.964279: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 15:14:23.964691: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00054, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 181ms/step - loss: 0.0027 - mean_absolute_error: 0.0300 - val_loss: 5.4435e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 2/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 7.3902e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 2: val_loss improved from 0.00054 to 0.00046, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 7.3902e-04 - mean_absolute_error: 0.0192 - val_loss: 4.6210e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 3/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.9466e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 3: val_loss improved from 0.00046 to 0.00044, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 6.9466e-04 - mean_absolute_error: 0.0185 - val_loss: 4.4453e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 4/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.9830e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 4: val_loss improved from 0.00044 to 0.00043, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 6.9830e-04 - mean_absolute_error: 0.0187 - val_loss: 4.2813e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 5/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.9607e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 5: val_loss did not improve from 0.00043\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 6.9607e-04 - mean_absolute_error: 0.0185 - val_loss: 4.3018e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 6/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.5006e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 6: val_loss did not improve from 0.00043\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 6.5006e-04 - mean_absolute_error: 0.0181 - val_loss: 9.1922e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 7/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.7234e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 7: val_loss did not improve from 0.00043\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 6.7234e-04 - mean_absolute_error: 0.0185 - val_loss: 4.8627e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 8/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 8.7214e-04 - mean_absolute_error: 0.0218\n",
      "Epoch 8: val_loss did not improve from 0.00043\n",
      "85/85 [==============================] - 16s 189ms/step - loss: 8.7214e-04 - mean_absolute_error: 0.0218 - val_loss: 5.6186e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 9/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8350e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 9: val_loss improved from 0.00043 to 0.00041, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 5.8350e-04 - mean_absolute_error: 0.0176 - val_loss: 4.1414e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 10/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.9283e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 10: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 194ms/step - loss: 5.9283e-04 - mean_absolute_error: 0.0177 - val_loss: 4.6756e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 11/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.2096e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 11: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 6.2096e-04 - mean_absolute_error: 0.0180 - val_loss: 4.6713e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 12/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.9936e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 12: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 5.9936e-04 - mean_absolute_error: 0.0175 - val_loss: 4.4173e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 13/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.0390e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 13: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 15s 181ms/step - loss: 6.0390e-04 - mean_absolute_error: 0.0175 - val_loss: 4.3456e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 14/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.9765e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 14: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 15s 182ms/step - loss: 5.9765e-04 - mean_absolute_error: 0.0178 - val_loss: 4.3299e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 15/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.3091e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 15: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 6.3091e-04 - mean_absolute_error: 0.0183 - val_loss: 4.5726e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 16/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.2501e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 16: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 6.2501e-04 - mean_absolute_error: 0.0180 - val_loss: 4.2597e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 17/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8796e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 17: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 5.8796e-04 - mean_absolute_error: 0.0176 - val_loss: 4.5199e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 18/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.6316e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 18: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 5.6316e-04 - mean_absolute_error: 0.0174 - val_loss: 4.5522e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 19/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8280e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 19: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 5.8280e-04 - mean_absolute_error: 0.0181 - val_loss: 4.5665e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 20/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.9311e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 20: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 5.9311e-04 - mean_absolute_error: 0.0181 - val_loss: 4.1457e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 21/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8161e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 21: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 5.8161e-04 - mean_absolute_error: 0.0178 - val_loss: 4.9386e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 22/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2587e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 22: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 191ms/step - loss: 5.2587e-04 - mean_absolute_error: 0.0174 - val_loss: 4.3548e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 23/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8523e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 23: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 186ms/step - loss: 5.8523e-04 - mean_absolute_error: 0.0180 - val_loss: 4.3450e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 24/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5010e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 24: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 186ms/step - loss: 5.5010e-04 - mean_absolute_error: 0.0177 - val_loss: 4.1746e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 25/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.7233e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 25: val_loss did not improve from 0.00041\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 5.7233e-04 - mean_absolute_error: 0.0181 - val_loss: 5.0828e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 26/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5850e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 26: val_loss improved from 0.00041 to 0.00040, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 187ms/step - loss: 5.5850e-04 - mean_absolute_error: 0.0180 - val_loss: 3.9803e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 27/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4323e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 27: val_loss improved from 0.00040 to 0.00039, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 5.4323e-04 - mean_absolute_error: 0.0179 - val_loss: 3.9468e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 28/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3848e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 28: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 5.3848e-04 - mean_absolute_error: 0.0179 - val_loss: 5.2861e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 29/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5870e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 29: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 5.5870e-04 - mean_absolute_error: 0.0180 - val_loss: 4.9198e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 30/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4071e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 30: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 5.4071e-04 - mean_absolute_error: 0.0178 - val_loss: 4.4370e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 31/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0009e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 31: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 5.0009e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0993e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 32/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5242e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 32: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 5.5242e-04 - mean_absolute_error: 0.0182 - val_loss: 4.1905e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 33/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2998e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 33: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 5.2998e-04 - mean_absolute_error: 0.0178 - val_loss: 4.0096e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 34/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3482e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 34: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 5.3482e-04 - mean_absolute_error: 0.0180 - val_loss: 4.9904e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 35/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4004e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 35: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 5.4004e-04 - mean_absolute_error: 0.0182 - val_loss: 3.9715e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 36/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4115e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 36: val_loss improved from 0.00039 to 0.00039, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 5.4115e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9313e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 37/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1105e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 37: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 5.1105e-04 - mean_absolute_error: 0.0177 - val_loss: 3.9646e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 38/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2961e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 38: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 5.2961e-04 - mean_absolute_error: 0.0181 - val_loss: 4.0771e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 39/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4582e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 39: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 17s 205ms/step - loss: 5.4582e-04 - mean_absolute_error: 0.0180 - val_loss: 4.2010e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 40/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3373e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 40: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 5.3373e-04 - mean_absolute_error: 0.0185 - val_loss: 4.1810e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 41/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5114e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 41: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 188ms/step - loss: 5.5114e-04 - mean_absolute_error: 0.0185 - val_loss: 3.9343e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 42/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0185e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 42: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 185ms/step - loss: 5.0185e-04 - mean_absolute_error: 0.0177 - val_loss: 5.1917e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 43/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3431e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 43: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 189ms/step - loss: 5.3431e-04 - mean_absolute_error: 0.0185 - val_loss: 4.4373e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 44/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9273e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 44: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 187ms/step - loss: 4.9273e-04 - mean_absolute_error: 0.0179 - val_loss: 4.3196e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 45/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4083e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 45: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 5.4083e-04 - mean_absolute_error: 0.0186 - val_loss: 5.1659e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 46/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3801e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 46: val_loss improved from 0.00039 to 0.00039, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 5.3801e-04 - mean_absolute_error: 0.0185 - val_loss: 3.9242e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 47/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2473e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 47: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 5.2473e-04 - mean_absolute_error: 0.0183 - val_loss: 4.0033e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 48/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8212e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 48: val_loss improved from 0.00039 to 0.00039, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 4.8212e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8670e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 49/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3301e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 49: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 5.3301e-04 - mean_absolute_error: 0.0184 - val_loss: 4.6999e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 50/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.9864e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 50: val_loss did not improve from 0.00039\n",
      "85/85 [==============================] - 18s 215ms/step - loss: 5.9864e-04 - mean_absolute_error: 0.0201 - val_loss: 4.0283e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 51/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8892e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 51: val_loss improved from 0.00039 to 0.00038, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 17s 204ms/step - loss: 4.8892e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8350e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 52/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9469e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 52: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 189ms/step - loss: 4.9469e-04 - mean_absolute_error: 0.0178 - val_loss: 4.0239e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 53/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8398e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 53: val_loss improved from 0.00038 to 0.00038, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 4.8398e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8111e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 54/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5256e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 54: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 194ms/step - loss: 5.5256e-04 - mean_absolute_error: 0.0190 - val_loss: 4.1240e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 55/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0392e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 55: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 5.0392e-04 - mean_absolute_error: 0.0183 - val_loss: 4.3039e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 56/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1409e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 56: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 5.1409e-04 - mean_absolute_error: 0.0188 - val_loss: 3.9193e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 57/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9553e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 57: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 189ms/step - loss: 4.9553e-04 - mean_absolute_error: 0.0184 - val_loss: 4.9440e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 58/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9674e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 58: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 4.9674e-04 - mean_absolute_error: 0.0186 - val_loss: 4.6656e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 59/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8683e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 59: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 4.8683e-04 - mean_absolute_error: 0.0182 - val_loss: 4.8211e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 60/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7652e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 60: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 4.7652e-04 - mean_absolute_error: 0.0180 - val_loss: 4.0507e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 61/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0429e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 61: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 189ms/step - loss: 5.0429e-04 - mean_absolute_error: 0.0181 - val_loss: 3.8431e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 62/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7392e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 62: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 18s 212ms/step - loss: 4.7392e-04 - mean_absolute_error: 0.0178 - val_loss: 3.9411e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 63/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9904e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 63: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 191ms/step - loss: 4.9904e-04 - mean_absolute_error: 0.0185 - val_loss: 4.2965e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 64/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1229e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 64: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 189ms/step - loss: 5.1229e-04 - mean_absolute_error: 0.0187 - val_loss: 4.1207e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 65/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0999e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 65: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 5.0999e-04 - mean_absolute_error: 0.0185 - val_loss: 3.8363e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 66/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4951e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 66: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 189ms/step - loss: 4.4951e-04 - mean_absolute_error: 0.0178 - val_loss: 3.9283e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 67/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0604e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 67: val_loss improved from 0.00038 to 0.00038, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 5.0604e-04 - mean_absolute_error: 0.0188 - val_loss: 3.7793e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 68/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2245e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 68: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 5.2245e-04 - mean_absolute_error: 0.0188 - val_loss: 4.1682e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 69/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9818e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 69: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 4.9818e-04 - mean_absolute_error: 0.0188 - val_loss: 4.6535e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 70/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1902e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 70: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 5.1902e-04 - mean_absolute_error: 0.0191 - val_loss: 3.9968e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 71/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7013e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 71: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 4.7013e-04 - mean_absolute_error: 0.0179 - val_loss: 4.5326e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 72/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9797e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 72: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 4.9797e-04 - mean_absolute_error: 0.0185 - val_loss: 3.7824e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 73/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9372e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 73: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 4.9372e-04 - mean_absolute_error: 0.0186 - val_loss: 4.0657e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 74/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9093e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 74: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 4.9093e-04 - mean_absolute_error: 0.0183 - val_loss: 4.1134e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 75/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0026e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 75: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 5.0026e-04 - mean_absolute_error: 0.0183 - val_loss: 3.7864e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 76/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4885e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 76: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 18s 207ms/step - loss: 4.4885e-04 - mean_absolute_error: 0.0176 - val_loss: 3.8347e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 77/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9536e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 77: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 4.9536e-04 - mean_absolute_error: 0.0184 - val_loss: 5.0353e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 78/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4848e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 78: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 5.4848e-04 - mean_absolute_error: 0.0194 - val_loss: 4.6066e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 79/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1725e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 79: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 191ms/step - loss: 5.1725e-04 - mean_absolute_error: 0.0189 - val_loss: 3.8532e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 80/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8643e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 80: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 4.8643e-04 - mean_absolute_error: 0.0182 - val_loss: 3.8235e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 81/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4358e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 81: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 4.4358e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7964e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 82/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0230e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 82: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 205ms/step - loss: 5.0230e-04 - mean_absolute_error: 0.0189 - val_loss: 4.0004e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 83/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7210e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 83: val_loss improved from 0.00038 to 0.00038, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 18s 207ms/step - loss: 4.7210e-04 - mean_absolute_error: 0.0179 - val_loss: 3.7737e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 84/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7994e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 84: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 204ms/step - loss: 4.7994e-04 - mean_absolute_error: 0.0177 - val_loss: 4.6623e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 85/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1282e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 85: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 18s 209ms/step - loss: 5.1282e-04 - mean_absolute_error: 0.0190 - val_loss: 3.9328e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 86/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5735e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 86: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 4.5735e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8949e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 87/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7209e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 87: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 4.7209e-04 - mean_absolute_error: 0.0180 - val_loss: 4.1167e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 88/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5945e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 88: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 4.5945e-04 - mean_absolute_error: 0.0177 - val_loss: 3.9162e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 89/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9369e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 89: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 4.9369e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9791e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 90/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9237e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 90: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 4.9237e-04 - mean_absolute_error: 0.0189 - val_loss: 3.8079e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 91/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8918e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 91: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 194ms/step - loss: 4.8918e-04 - mean_absolute_error: 0.0185 - val_loss: 3.8130e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 92/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1899e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 92: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 5.1899e-04 - mean_absolute_error: 0.0186 - val_loss: 4.6932e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 93/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8603e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 93: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 16s 191ms/step - loss: 4.8603e-04 - mean_absolute_error: 0.0182 - val_loss: 4.2072e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 94/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7570e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 94: val_loss improved from 0.00038 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 4.7570e-04 - mean_absolute_error: 0.0180 - val_loss: 3.7466e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 95/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7247e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 95: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 4.7247e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8616e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 96/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7395e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 96: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 191ms/step - loss: 4.7395e-04 - mean_absolute_error: 0.0181 - val_loss: 4.8839e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 97/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8170e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 97: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 4.8170e-04 - mean_absolute_error: 0.0182 - val_loss: 4.0308e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 98/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7276e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 98: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 4.7276e-04 - mean_absolute_error: 0.0183 - val_loss: 4.6061e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 99/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1188e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 99: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 5.1188e-04 - mean_absolute_error: 0.0189 - val_loss: 6.1925e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 100/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9846e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 100: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 4.9846e-04 - mean_absolute_error: 0.0185 - val_loss: 4.0628e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 101/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7784e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 101: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 4.7784e-04 - mean_absolute_error: 0.0183 - val_loss: 3.7405e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 102/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6015e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 102: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 4.6015e-04 - mean_absolute_error: 0.0180 - val_loss: 3.8406e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 103/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9095e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 103: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 4.9095e-04 - mean_absolute_error: 0.0182 - val_loss: 3.8768e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 104/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8258e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 104: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 4.8258e-04 - mean_absolute_error: 0.0182 - val_loss: 5.0199e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 105/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8547e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 105: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 191ms/step - loss: 4.8547e-04 - mean_absolute_error: 0.0184 - val_loss: 3.7527e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 106/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8656e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 106: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 4.8656e-04 - mean_absolute_error: 0.0184 - val_loss: 4.4614e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 107/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5110e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 107: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 191ms/step - loss: 4.5110e-04 - mean_absolute_error: 0.0178 - val_loss: 4.0915e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 108/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6996e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 108: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 4.6996e-04 - mean_absolute_error: 0.0180 - val_loss: 4.0032e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 109/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6639e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 109: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 4.6639e-04 - mean_absolute_error: 0.0177 - val_loss: 3.9347e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 110/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8063e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 110: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 4.8063e-04 - mean_absolute_error: 0.0186 - val_loss: 4.0627e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 111/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6666e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 111: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 190ms/step - loss: 4.6666e-04 - mean_absolute_error: 0.0180 - val_loss: 3.9906e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 112/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7954e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 112: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 4.7954e-04 - mean_absolute_error: 0.0185 - val_loss: 4.1632e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 113/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9935e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 113: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 4.9935e-04 - mean_absolute_error: 0.0189 - val_loss: 3.8031e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 114/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6006e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 114: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 222ms/step - loss: 4.6006e-04 - mean_absolute_error: 0.0178 - val_loss: 4.5849e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 115/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3758e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 115: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 235ms/step - loss: 4.3758e-04 - mean_absolute_error: 0.0173 - val_loss: 4.3131e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 116/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7688e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 116: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.7688e-04 - mean_absolute_error: 0.0179 - val_loss: 4.3366e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 117/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2401e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 117: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 266ms/step - loss: 5.2401e-04 - mean_absolute_error: 0.0190 - val_loss: 4.6738e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 118/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8309e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 118: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 265ms/step - loss: 4.8309e-04 - mean_absolute_error: 0.0184 - val_loss: 4.2780e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 119/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6103e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 119: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 263ms/step - loss: 4.6103e-04 - mean_absolute_error: 0.0178 - val_loss: 4.4452e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 120/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0199e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 120: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 232ms/step - loss: 5.0199e-04 - mean_absolute_error: 0.0181 - val_loss: 3.9686e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 121/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4974e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 121: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 252ms/step - loss: 4.4974e-04 - mean_absolute_error: 0.0175 - val_loss: 3.9699e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 122/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4989e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 122: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 246ms/step - loss: 4.4989e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8869e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 123/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9243e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 123: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 244ms/step - loss: 4.9243e-04 - mean_absolute_error: 0.0185 - val_loss: 4.0669e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 124/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8544e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 124: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 4.8544e-04 - mean_absolute_error: 0.0181 - val_loss: 3.8135e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 125/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7154e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 125: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 259ms/step - loss: 4.7154e-04 - mean_absolute_error: 0.0182 - val_loss: 3.7605e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 126/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5982e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 126: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 4.5982e-04 - mean_absolute_error: 0.0177 - val_loss: 3.9945e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 127/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8754e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 127: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 4.8754e-04 - mean_absolute_error: 0.0182 - val_loss: 3.7489e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 128/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7735e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 128: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 243ms/step - loss: 4.7735e-04 - mean_absolute_error: 0.0183 - val_loss: 3.8073e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 129/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9140e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 129: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 244ms/step - loss: 4.9140e-04 - mean_absolute_error: 0.0183 - val_loss: 4.3288e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 130/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5516e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 130: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 242ms/step - loss: 4.5516e-04 - mean_absolute_error: 0.0179 - val_loss: 4.3453e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 131/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6270e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 131: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 239ms/step - loss: 4.6270e-04 - mean_absolute_error: 0.0180 - val_loss: 4.1994e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 132/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4801e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 132: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 250ms/step - loss: 4.4801e-04 - mean_absolute_error: 0.0175 - val_loss: 3.8209e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 133/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5096e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 133: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 241ms/step - loss: 4.5096e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0027e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 134/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6510e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 134: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 235ms/step - loss: 4.6510e-04 - mean_absolute_error: 0.0177 - val_loss: 3.7570e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 135/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5487e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 135: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.5487e-04 - mean_absolute_error: 0.0174 - val_loss: 3.9095e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 136/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8859e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 136: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 240ms/step - loss: 4.8859e-04 - mean_absolute_error: 0.0186 - val_loss: 5.1125e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 137/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8128e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 137: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.8128e-04 - mean_absolute_error: 0.0182 - val_loss: 3.7387e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 138/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6793e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 138: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 264ms/step - loss: 4.6793e-04 - mean_absolute_error: 0.0176 - val_loss: 4.1491e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 139/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6162e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 139: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 241ms/step - loss: 4.6162e-04 - mean_absolute_error: 0.0182 - val_loss: 4.2510e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 140/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5641e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 140: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 244ms/step - loss: 4.5641e-04 - mean_absolute_error: 0.0176 - val_loss: 3.9585e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 141/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6950e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 141: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 252ms/step - loss: 4.6950e-04 - mean_absolute_error: 0.0177 - val_loss: 3.8196e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 142/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6595e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 142: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 247ms/step - loss: 4.6595e-04 - mean_absolute_error: 0.0180 - val_loss: 3.8582e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 143/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7596e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 143: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 243ms/step - loss: 4.7596e-04 - mean_absolute_error: 0.0182 - val_loss: 4.6831e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 144/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3924e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 144: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.3924e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7265e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 145/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5008e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 145: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 246ms/step - loss: 4.5008e-04 - mean_absolute_error: 0.0176 - val_loss: 3.9312e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 146/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7648e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 146: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 240ms/step - loss: 4.7648e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8001e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 147/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9213e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 147: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 249ms/step - loss: 4.9213e-04 - mean_absolute_error: 0.0182 - val_loss: 4.1811e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 148/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9902e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 148: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 252ms/step - loss: 4.9902e-04 - mean_absolute_error: 0.0183 - val_loss: 3.7335e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 149/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7039e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 149: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.7039e-04 - mean_absolute_error: 0.0178 - val_loss: 4.4686e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 150/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1165e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 150: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 236ms/step - loss: 5.1165e-04 - mean_absolute_error: 0.0187 - val_loss: 3.8705e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 151/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6095e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 151: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 222ms/step - loss: 4.6095e-04 - mean_absolute_error: 0.0179 - val_loss: 4.4884e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 152/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7539e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 152: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 228ms/step - loss: 4.7539e-04 - mean_absolute_error: 0.0183 - val_loss: 4.0043e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 153/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1030e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 153: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 228ms/step - loss: 5.1030e-04 - mean_absolute_error: 0.0190 - val_loss: 3.8931e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 154/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3957e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 154: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 230ms/step - loss: 4.3957e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7333e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 155/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3912e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 155: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.3912e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7890e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 156/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5796e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 156: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 238ms/step - loss: 4.5796e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7268e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 157/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7132e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 157: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 228ms/step - loss: 4.7132e-04 - mean_absolute_error: 0.0178 - val_loss: 3.9484e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 158/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6919e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 158: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 230ms/step - loss: 4.6919e-04 - mean_absolute_error: 0.0179 - val_loss: 4.1876e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 159/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8197e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 159: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 231ms/step - loss: 4.8197e-04 - mean_absolute_error: 0.0183 - val_loss: 4.0151e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 160/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4282e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 160: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 226ms/step - loss: 4.4282e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8925e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 161/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8769e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 161: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 19s 228ms/step - loss: 4.8769e-04 - mean_absolute_error: 0.0181 - val_loss: 4.1137e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 162/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8115e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 162: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 232ms/step - loss: 4.8115e-04 - mean_absolute_error: 0.0181 - val_loss: 3.9497e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 163/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4682e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 163: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 240ms/step - loss: 4.4682e-04 - mean_absolute_error: 0.0173 - val_loss: 3.9175e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 164/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3329e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 164: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 4.3329e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7681e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 165/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3790e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 165: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 246ms/step - loss: 4.3790e-04 - mean_absolute_error: 0.0171 - val_loss: 4.3224e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 166/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3403e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 166: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 241ms/step - loss: 4.3403e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7965e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 167/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5977e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 167: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 242ms/step - loss: 4.5977e-04 - mean_absolute_error: 0.0178 - val_loss: 4.0100e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 168/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5904e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 168: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 249ms/step - loss: 4.5904e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8001e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 169/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6461e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 169: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 237ms/step - loss: 4.6461e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8627e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 170/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6227e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 170: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 239ms/step - loss: 4.6227e-04 - mean_absolute_error: 0.0177 - val_loss: 4.1870e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 171/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5203e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 171: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 4.5203e-04 - mean_absolute_error: 0.0176 - val_loss: 4.0002e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 172/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7212e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 172: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 242ms/step - loss: 4.7212e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8301e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 173/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6291e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 173: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 242ms/step - loss: 4.6291e-04 - mean_absolute_error: 0.0177 - val_loss: 4.0682e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 174/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6542e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 174: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 237ms/step - loss: 4.6542e-04 - mean_absolute_error: 0.0177 - val_loss: 3.9735e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 175/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7067e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 175: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 240ms/step - loss: 4.7067e-04 - mean_absolute_error: 0.0179 - val_loss: 3.9253e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 176/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3990e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 176: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 247ms/step - loss: 4.3990e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7318e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 177/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6222e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 177: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 247ms/step - loss: 4.6222e-04 - mean_absolute_error: 0.0174 - val_loss: 4.0726e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 178/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4556e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 178: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 240ms/step - loss: 4.4556e-04 - mean_absolute_error: 0.0173 - val_loss: 4.4355e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 179/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5152e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 179: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 241ms/step - loss: 4.5152e-04 - mean_absolute_error: 0.0175 - val_loss: 3.8850e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 180/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6752e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 180: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 242ms/step - loss: 4.6752e-04 - mean_absolute_error: 0.0175 - val_loss: 4.6315e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 181/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6585e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 181: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 263ms/step - loss: 4.6585e-04 - mean_absolute_error: 0.0179 - val_loss: 3.9780e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 182/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8017e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 182: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.8017e-04 - mean_absolute_error: 0.0179 - val_loss: 4.6889e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 183/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4192e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 183: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 91s 1s/step - loss: 4.4192e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7549e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 184/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4604e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 184: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 250ms/step - loss: 4.4604e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8450e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 185/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5599e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 185: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.5599e-04 - mean_absolute_error: 0.0176 - val_loss: 3.8404e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 186/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8150e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 186: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 94s 1s/step - loss: 4.8150e-04 - mean_absolute_error: 0.0181 - val_loss: 3.8694e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 187/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3180e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 187: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 89s 1s/step - loss: 4.3180e-04 - mean_absolute_error: 0.0172 - val_loss: 3.9292e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 188/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3108e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 188: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 270ms/step - loss: 4.3108e-04 - mean_absolute_error: 0.0174 - val_loss: 3.9823e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 189/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3234e-04 - mean_absolute_error: 0.0172 \n",
      "Epoch 189: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 1287s 15s/step - loss: 4.3234e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7289e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 190/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6127e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 190: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.6127e-04 - mean_absolute_error: 0.0174 - val_loss: 3.9128e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 191/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3222e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 191: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.3222e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7666e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 192/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3871e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 192: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.3871e-04 - mean_absolute_error: 0.0173 - val_loss: 3.9012e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 193/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4729e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 193: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 253ms/step - loss: 4.4729e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8793e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 194/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4470e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 194: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.4470e-04 - mean_absolute_error: 0.0173 - val_loss: 3.9053e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 195/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4081e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 195: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 252ms/step - loss: 4.4081e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7866e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 196/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4412e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 196: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.4412e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8074e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 197/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7374e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 197: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.7374e-04 - mean_absolute_error: 0.0180 - val_loss: 4.1944e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 198/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4548e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 198: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 250ms/step - loss: 4.4548e-04 - mean_absolute_error: 0.0172 - val_loss: 3.9008e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 199/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7251e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 199: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.7251e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8874e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 200/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3757e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 200: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 252ms/step - loss: 4.3757e-04 - mean_absolute_error: 0.0175 - val_loss: 3.8533e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 201/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5121e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 201: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.5121e-04 - mean_absolute_error: 0.0175 - val_loss: 3.8675e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 202/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3973e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 202: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.3973e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7207e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 203/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5671e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 203: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.5671e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8568e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 204/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3333e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 204: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.3333e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8219e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 205/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3824e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 205: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.3824e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7388e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 206/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4690e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 206: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 236ms/step - loss: 4.4690e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7860e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 207/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2975e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 207: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 265ms/step - loss: 4.2975e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7499e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 208/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2267e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 208: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 267ms/step - loss: 4.2267e-04 - mean_absolute_error: 0.0169 - val_loss: 4.1550e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 209/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3432e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 209: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.3432e-04 - mean_absolute_error: 0.0170 - val_loss: 4.0533e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 210/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3437e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 210: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.3437e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7427e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 211/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3851e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 211: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.3851e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8362e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 212/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4956e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 212: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.4956e-04 - mean_absolute_error: 0.0174 - val_loss: 4.1453e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 213/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7460e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 213: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.7460e-04 - mean_absolute_error: 0.0180 - val_loss: 3.7989e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 214/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5455e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 214: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.5455e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8298e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 215/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4966e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 215: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.4966e-04 - mean_absolute_error: 0.0173 - val_loss: 5.2718e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 216/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2946e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 216: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.2946e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7931e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 217/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2015e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 217: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.2015e-04 - mean_absolute_error: 0.0168 - val_loss: 4.0467e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 218/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4775e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 218: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.4775e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0068e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 219/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5578e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 219: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.5578e-04 - mean_absolute_error: 0.0179 - val_loss: 4.2350e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 220/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4799e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 220: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.4799e-04 - mean_absolute_error: 0.0174 - val_loss: 3.9460e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 221/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5412e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 221: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.5412e-04 - mean_absolute_error: 0.0174 - val_loss: 4.2518e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 222/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4957e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 222: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.4957e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8338e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 223/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2923e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 223: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 250ms/step - loss: 4.2923e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7867e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 224/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3341e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 224: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.3341e-04 - mean_absolute_error: 0.0171 - val_loss: 4.6188e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 225/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5229e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 225: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 249ms/step - loss: 4.5229e-04 - mean_absolute_error: 0.0174 - val_loss: 3.9826e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 226/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5149e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 226: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 239ms/step - loss: 4.5149e-04 - mean_absolute_error: 0.0176 - val_loss: 4.0636e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 227/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2882e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 227: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 239ms/step - loss: 4.2882e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8058e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 228/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8255e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 228: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 234ms/step - loss: 4.8255e-04 - mean_absolute_error: 0.0178 - val_loss: 4.1728e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 229/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3674e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 229: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 233ms/step - loss: 4.3674e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7412e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 230/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2689e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 230: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 4.2689e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8112e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 231/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3832e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 231: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 243ms/step - loss: 4.3832e-04 - mean_absolute_error: 0.0171 - val_loss: 3.8107e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 232/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4083e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 232: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 4.4083e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8957e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 233/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4289e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 233: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 247ms/step - loss: 4.4289e-04 - mean_absolute_error: 0.0173 - val_loss: 5.0654e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 234/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5848e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 234: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.5848e-04 - mean_absolute_error: 0.0173 - val_loss: 4.7088e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 235/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4337e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 235: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 20s 241ms/step - loss: 4.4337e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7597e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 236/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4262e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 236: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.4262e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7547e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 237/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7816e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 237: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 4.7816e-04 - mean_absolute_error: 0.0180 - val_loss: 4.0011e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 238/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6254e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 238: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.6254e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8285e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 239/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5116e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 239: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.5116e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8906e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 240/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3530e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 240: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.3530e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7228e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 241/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2902e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 241: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.2902e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8327e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 242/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2791e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 242: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.2791e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8061e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 243/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4492e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 243: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 259ms/step - loss: 4.4492e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7957e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 244/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7135e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 244: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.7135e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7558e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 245/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5312e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 245: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.5312e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8402e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 246/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5686e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 246: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 249ms/step - loss: 4.5686e-04 - mean_absolute_error: 0.0176 - val_loss: 4.1083e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 247/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4171e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 247: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 21s 249ms/step - loss: 4.4171e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7109e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 248/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4383e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 248: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.4383e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7223e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 249/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3958e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 249: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 4.3958e-04 - mean_absolute_error: 0.0174 - val_loss: 3.9139e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 250/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4599e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 250: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 259ms/step - loss: 4.4599e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7411e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 251/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4376e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 251: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.4376e-04 - mean_absolute_error: 0.0175 - val_loss: 3.8369e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 252/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3032e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 252: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 249ms/step - loss: 4.3032e-04 - mean_absolute_error: 0.0169 - val_loss: 4.0836e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 253/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1554e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 253: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 4.1554e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7278e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 254/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3230e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 254: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.3230e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8633e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 255/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4326e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 255: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 4.4326e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8403e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 256/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4299e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 256: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 4.4299e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7206e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 257/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5815e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 257: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 246ms/step - loss: 4.5815e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8344e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 258/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5232e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 258: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 247ms/step - loss: 4.5232e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7498e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 259/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2829e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 259: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.2829e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7275e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 260/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5154e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 260: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 259ms/step - loss: 4.5154e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7885e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 261/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2406e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 261: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 247ms/step - loss: 4.2406e-04 - mean_absolute_error: 0.0169 - val_loss: 4.2078e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 262/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7241e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 262: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.7241e-04 - mean_absolute_error: 0.0178 - val_loss: 4.4165e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 263/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7658e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 263: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.7658e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8642e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 264/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3670e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 264: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 279ms/step - loss: 4.3670e-04 - mean_absolute_error: 0.0174 - val_loss: 3.9083e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 265/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5950e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 265: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 267ms/step - loss: 4.5950e-04 - mean_absolute_error: 0.0176 - val_loss: 3.8571e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 266/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2993e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 266: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.2993e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7847e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 267/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4671e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 267: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 263ms/step - loss: 4.4671e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7813e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 268/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3971e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 268: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 253ms/step - loss: 4.3971e-04 - mean_absolute_error: 0.0172 - val_loss: 3.9459e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 269/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4235e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 269: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.4235e-04 - mean_absolute_error: 0.0174 - val_loss: 4.0750e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 270/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3074e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 270: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 261ms/step - loss: 4.3074e-04 - mean_absolute_error: 0.0172 - val_loss: 3.9489e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 271/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6610e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 271: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 261ms/step - loss: 4.6610e-04 - mean_absolute_error: 0.0178 - val_loss: 3.7192e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 272/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2699e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 272: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 263ms/step - loss: 4.2699e-04 - mean_absolute_error: 0.0169 - val_loss: 4.4074e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 273/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5951e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 273: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.5951e-04 - mean_absolute_error: 0.0176 - val_loss: 3.9405e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 274/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3831e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 274: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 264ms/step - loss: 4.3831e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8046e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 275/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4222e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 275: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 280ms/step - loss: 4.4222e-04 - mean_absolute_error: 0.0173 - val_loss: 3.9906e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 276/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3790e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 276: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 285ms/step - loss: 4.3790e-04 - mean_absolute_error: 0.0171 - val_loss: 3.8102e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 277/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3186e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 277: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 297ms/step - loss: 4.3186e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8484e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 278/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3128e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 278: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 289ms/step - loss: 4.3128e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7602e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 279/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3537e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 279: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 282ms/step - loss: 4.3537e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7726e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 280/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2480e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 280: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 282ms/step - loss: 4.2480e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7600e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 281/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5728e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 281: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 24s 283ms/step - loss: 4.5728e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7071e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 282/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2743e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 282: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 293ms/step - loss: 4.2743e-04 - mean_absolute_error: 0.0168 - val_loss: 4.1789e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 283/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4643e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 283: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 298ms/step - loss: 4.4643e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7887e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 284/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3598e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 284: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 26s 301ms/step - loss: 4.3598e-04 - mean_absolute_error: 0.0171 - val_loss: 3.9179e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 285/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2597e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 285: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 289ms/step - loss: 4.2597e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7518e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 286/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4778e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 286: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 294ms/step - loss: 4.4778e-04 - mean_absolute_error: 0.0171 - val_loss: 3.9800e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 287/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4305e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 287: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 293ms/step - loss: 4.4305e-04 - mean_absolute_error: 0.0173 - val_loss: 3.9414e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 288/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5409e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 288: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 4.5409e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7928e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 289/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2875e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 289: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 290ms/step - loss: 4.2875e-04 - mean_absolute_error: 0.0172 - val_loss: 4.0218e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 290/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4726e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 290: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 281ms/step - loss: 4.4726e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8774e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 291/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6572e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 291: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 288ms/step - loss: 4.6572e-04 - mean_absolute_error: 0.0175 - val_loss: 3.8039e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 292/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6264e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 292: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 284ms/step - loss: 4.6264e-04 - mean_absolute_error: 0.0175 - val_loss: 3.9990e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 293/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3666e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 293: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 290ms/step - loss: 4.3666e-04 - mean_absolute_error: 0.0172 - val_loss: 4.1431e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 294/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3318e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 294: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 44s 525ms/step - loss: 4.3318e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7267e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 295/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3012e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 295: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 51s 602ms/step - loss: 4.3012e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8167e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 296/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3795e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 296: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 53s 619ms/step - loss: 4.3795e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7334e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 297/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2666e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 297: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 52s 618ms/step - loss: 4.2666e-04 - mean_absolute_error: 0.0171 - val_loss: 3.9097e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 298/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2243e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 298: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 51s 604ms/step - loss: 4.2243e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7076e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 299/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0566e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 299: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 49s 572ms/step - loss: 4.0566e-04 - mean_absolute_error: 0.0166 - val_loss: 3.9677e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 300/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4807e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 300: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 54s 636ms/step - loss: 4.4807e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8018e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 301/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4043e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 301: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 55s 642ms/step - loss: 4.4043e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8199e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 302/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1130e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 302: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 48s 565ms/step - loss: 4.1130e-04 - mean_absolute_error: 0.0167 - val_loss: 3.9852e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 303/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1070e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 303: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 54s 634ms/step - loss: 4.1070e-04 - mean_absolute_error: 0.0166 - val_loss: 3.7437e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 304/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3328e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 304: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 59s 701ms/step - loss: 4.3328e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8206e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 305/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2205e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 305: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 57s 675ms/step - loss: 4.2205e-04 - mean_absolute_error: 0.0168 - val_loss: 4.0722e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 306/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5050e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 306: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 69s 810ms/step - loss: 4.5050e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7646e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 307/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3973e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 307: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 55s 652ms/step - loss: 4.3973e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7564e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 308/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2806e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 308: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 156s 2s/step - loss: 4.2806e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7597e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 309/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2044e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 309: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 48s 571ms/step - loss: 4.2044e-04 - mean_absolute_error: 0.0169 - val_loss: 4.7775e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 310/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5359e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 310: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 48s 568ms/step - loss: 4.5359e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8536e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 311/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1935e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 311: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 47s 558ms/step - loss: 4.1935e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7287e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 312/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3016e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 312: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 31s 363ms/step - loss: 4.3016e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8914e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 313/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5152e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 313: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.5152e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8835e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 314/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2058e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 314: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 270ms/step - loss: 4.2058e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7292e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 315/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1931e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 315: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 23s 273ms/step - loss: 4.1931e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6937e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 316/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3920e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 316: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 278ms/step - loss: 4.3920e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8868e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 317/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2873e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 317: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 300ms/step - loss: 4.2873e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8493e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 318/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0716e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 318: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 267ms/step - loss: 4.0716e-04 - mean_absolute_error: 0.0166 - val_loss: 3.9396e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 319/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5226e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 319: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 27s 314ms/step - loss: 4.5226e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0287e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 320/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2507e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 320: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 297ms/step - loss: 4.2507e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7851e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 321/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2716e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 321: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 294ms/step - loss: 4.2716e-04 - mean_absolute_error: 0.0170 - val_loss: 4.3911e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 322/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4286e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 322: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 288ms/step - loss: 4.4286e-04 - mean_absolute_error: 0.0172 - val_loss: 3.7562e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 323/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2529e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 323: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 27s 316ms/step - loss: 4.2529e-04 - mean_absolute_error: 0.0172 - val_loss: 4.2154e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 324/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2031e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 324: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 278ms/step - loss: 4.2031e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7937e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 325/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1646e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 325: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 26s 304ms/step - loss: 4.1646e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7595e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 326/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5109e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 326: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 28s 328ms/step - loss: 4.5109e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8394e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 327/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0049e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 327: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 26s 308ms/step - loss: 4.0049e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6922e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 328/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3968e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 328: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 26s 301ms/step - loss: 4.3968e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7029e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 329/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0471e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 329: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 287ms/step - loss: 4.0471e-04 - mean_absolute_error: 0.0165 - val_loss: 3.9346e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 330/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1601e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 330: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 294ms/step - loss: 4.1601e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8969e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 331/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3552e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 331: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 280ms/step - loss: 4.3552e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7903e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 332/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2814e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 332: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 299ms/step - loss: 4.2814e-04 - mean_absolute_error: 0.0168 - val_loss: 3.9140e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 333/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2454e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 333: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 295ms/step - loss: 4.2454e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8470e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 334/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5205e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 334: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 296ms/step - loss: 4.5205e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0427e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 335/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3399e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 335: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.3399e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0003e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 336/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2955e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 336: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 275ms/step - loss: 4.2955e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7314e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 337/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2891e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 337: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.2891e-04 - mean_absolute_error: 0.0167 - val_loss: 3.9863e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 338/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4230e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 338: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 271ms/step - loss: 4.4230e-04 - mean_absolute_error: 0.0171 - val_loss: 3.8220e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 339/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2981e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 339: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.2981e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7164e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 340/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2045e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 340: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 266ms/step - loss: 4.2045e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8206e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 341/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2552e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 341: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 274ms/step - loss: 4.2552e-04 - mean_absolute_error: 0.0168 - val_loss: 3.9138e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 342/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2616e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 342: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 26s 303ms/step - loss: 4.2616e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7106e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 343/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1059e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 343: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 263ms/step - loss: 4.1059e-04 - mean_absolute_error: 0.0166 - val_loss: 4.0217e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 344/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1752e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 344: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 284ms/step - loss: 4.1752e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8856e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 345/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1077e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 345: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 4.1077e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7033e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 346/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4174e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 346: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.4174e-04 - mean_absolute_error: 0.0171 - val_loss: 4.9111e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 347/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3543e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 347: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.3543e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7966e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 348/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4247e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 348: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.4247e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7582e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 349/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3500e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 349: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.3500e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7358e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 350/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4262e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 350: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.4262e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7422e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 351/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0706e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 351: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 4.0706e-04 - mean_absolute_error: 0.0165 - val_loss: 4.1317e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 352/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3979e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 352: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 259ms/step - loss: 4.3979e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7348e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 353/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2684e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 353: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.2684e-04 - mean_absolute_error: 0.0170 - val_loss: 4.0406e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 354/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1087e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 354: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 263ms/step - loss: 4.1087e-04 - mean_absolute_error: 0.0167 - val_loss: 3.8704e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 355/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4285e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 355: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 261ms/step - loss: 4.4285e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7332e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 356/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3720e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 356: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.3720e-04 - mean_absolute_error: 0.0169 - val_loss: 3.9010e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 357/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0532e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 357: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 277ms/step - loss: 4.0532e-04 - mean_absolute_error: 0.0165 - val_loss: 3.7801e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 358/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0715e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 358: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 27s 312ms/step - loss: 4.0715e-04 - mean_absolute_error: 0.0165 - val_loss: 3.7315e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 359/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4434e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 359: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 285ms/step - loss: 4.4434e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7054e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 360/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1498e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 360: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 265ms/step - loss: 4.1498e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7107e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 361/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2337e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 361: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 265ms/step - loss: 4.2337e-04 - mean_absolute_error: 0.0166 - val_loss: 3.7333e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 362/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2533e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 362: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.2533e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8151e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 363/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3487e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 363: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.3487e-04 - mean_absolute_error: 0.0170 - val_loss: 3.9306e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 364/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1427e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 364: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.1427e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7879e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 365/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2569e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 365: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.2569e-04 - mean_absolute_error: 0.0168 - val_loss: 3.9514e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 366/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2109e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 366: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 263ms/step - loss: 4.2109e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8121e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 367/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4089e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 367: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.4089e-04 - mean_absolute_error: 0.0171 - val_loss: 3.8302e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 368/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0994e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 368: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 4.0994e-04 - mean_absolute_error: 0.0165 - val_loss: 4.2192e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 369/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2737e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 369: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 267ms/step - loss: 4.2737e-04 - mean_absolute_error: 0.0168 - val_loss: 4.0029e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 370/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0750e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 370: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 265ms/step - loss: 4.0750e-04 - mean_absolute_error: 0.0165 - val_loss: 3.7385e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 371/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2041e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 371: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 274ms/step - loss: 4.2041e-04 - mean_absolute_error: 0.0166 - val_loss: 4.1623e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 372/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3607e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 372: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.3607e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8731e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 373/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1446e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 373: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 286ms/step - loss: 4.1446e-04 - mean_absolute_error: 0.0169 - val_loss: 4.1351e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 374/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1211e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 374: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 276ms/step - loss: 4.1211e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7820e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 375/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1474e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 375: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 286ms/step - loss: 4.1474e-04 - mean_absolute_error: 0.0166 - val_loss: 4.4774e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 376/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3142e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 376: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 276ms/step - loss: 4.3142e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7045e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 377/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1810e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 377: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 273ms/step - loss: 4.1810e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8457e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 378/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0612e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 378: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 288ms/step - loss: 4.0612e-04 - mean_absolute_error: 0.0166 - val_loss: 3.8814e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 379/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4526e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 379: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 284ms/step - loss: 4.4526e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7796e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 380/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1414e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 380: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 268ms/step - loss: 4.1414e-04 - mean_absolute_error: 0.0166 - val_loss: 3.7955e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 381/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2361e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 381: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 273ms/step - loss: 4.2361e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7378e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 382/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4271e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 382: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.4271e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7698e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 383/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3214e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 383: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 271ms/step - loss: 4.3214e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7164e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 384/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1935e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 384: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 271ms/step - loss: 4.1935e-04 - mean_absolute_error: 0.0170 - val_loss: 4.0781e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 385/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3277e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 385: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.3277e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8820e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 386/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2380e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 386: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 272ms/step - loss: 4.2380e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8170e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 387/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4054e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 387: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 270ms/step - loss: 4.4054e-04 - mean_absolute_error: 0.0171 - val_loss: 4.0955e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 388/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2062e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 388: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 270ms/step - loss: 4.2062e-04 - mean_absolute_error: 0.0165 - val_loss: 3.8063e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 389/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2451e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 389: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.2451e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8400e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 390/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1657e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 390: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 294ms/step - loss: 4.1657e-04 - mean_absolute_error: 0.0166 - val_loss: 3.7041e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 391/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2921e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 391: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 285ms/step - loss: 4.2921e-04 - mean_absolute_error: 0.0172 - val_loss: 4.2214e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 392/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2982e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 392: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 277ms/step - loss: 4.2982e-04 - mean_absolute_error: 0.0168 - val_loss: 3.9344e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 393/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1986e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 393: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 25s 293ms/step - loss: 4.1986e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6767e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 394/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4766e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 394: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 295ms/step - loss: 4.4766e-04 - mean_absolute_error: 0.0171 - val_loss: 4.5676e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 395/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3664e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 395: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 26s 310ms/step - loss: 4.3664e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6856e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 396/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1606e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 396: val_loss improved from 0.00037 to 0.00037, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 28s 326ms/step - loss: 4.1606e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6520e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 397/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0642e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 397: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 277ms/step - loss: 4.0642e-04 - mean_absolute_error: 0.0165 - val_loss: 4.1617e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 398/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1567e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 398: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 259ms/step - loss: 4.1567e-04 - mean_absolute_error: 0.0167 - val_loss: 4.1476e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 399/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1998e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 399: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 281ms/step - loss: 4.1998e-04 - mean_absolute_error: 0.0167 - val_loss: 4.2409e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 400/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2866e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 400: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 276ms/step - loss: 4.2866e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6852e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 401/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4205e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 401: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 283ms/step - loss: 4.4205e-04 - mean_absolute_error: 0.0170 - val_loss: 3.6880e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 402/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2229e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 402: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 288ms/step - loss: 4.2229e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7095e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 403/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9904e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 403: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 282ms/step - loss: 3.9904e-04 - mean_absolute_error: 0.0164 - val_loss: 3.7932e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 404/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3658e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 404: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 299ms/step - loss: 4.3658e-04 - mean_absolute_error: 0.0173 - val_loss: 3.6860e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 405/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3039e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 405: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 26s 301ms/step - loss: 4.3039e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8322e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 406/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2336e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 406: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 268ms/step - loss: 4.2336e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7311e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 407/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2051e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 407: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 4.2051e-04 - mean_absolute_error: 0.0169 - val_loss: 4.1800e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 408/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2045e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 408: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 21s 252ms/step - loss: 4.2045e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8484e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 409/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2930e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 409: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 4.2930e-04 - mean_absolute_error: 0.0170 - val_loss: 3.9577e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 410/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2857e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 410: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 266ms/step - loss: 4.2857e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7380e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 411/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3579e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 411: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 268ms/step - loss: 4.3579e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8737e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 412/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3061e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 412: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 286ms/step - loss: 4.3061e-04 - mean_absolute_error: 0.0171 - val_loss: 3.9129e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 413/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3242e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 413: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 299ms/step - loss: 4.3242e-04 - mean_absolute_error: 0.0169 - val_loss: 3.8182e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 414/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1388e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 414: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 25s 296ms/step - loss: 4.1388e-04 - mean_absolute_error: 0.0167 - val_loss: 4.3776e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 415/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1464e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 415: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 24s 279ms/step - loss: 4.1464e-04 - mean_absolute_error: 0.0168 - val_loss: 3.9126e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 416/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0872e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 416: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 268ms/step - loss: 4.0872e-04 - mean_absolute_error: 0.0167 - val_loss: 3.9768e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 417/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2456e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 417: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 265ms/step - loss: 4.2456e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7449e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 418/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0437e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 418: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 271ms/step - loss: 4.0437e-04 - mean_absolute_error: 0.0168 - val_loss: 3.9021e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 419/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9899e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 419: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 273ms/step - loss: 3.9899e-04 - mean_absolute_error: 0.0166 - val_loss: 3.7179e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 420/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1584e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 420: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 23s 268ms/step - loss: 4.1584e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7968e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 421/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1411e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 421: val_loss did not improve from 0.00037\n",
      "85/85 [==============================] - 22s 260ms/step - loss: 4.1411e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6836e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 422/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2989e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 422: val_loss improved from 0.00037 to 0.00036, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 26s 301ms/step - loss: 4.2989e-04 - mean_absolute_error: 0.0168 - val_loss: 3.6468e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 423/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1420e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 423: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 24s 283ms/step - loss: 4.1420e-04 - mean_absolute_error: 0.0166 - val_loss: 3.8420e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 424/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2438e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 424: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 22s 261ms/step - loss: 4.2438e-04 - mean_absolute_error: 0.0168 - val_loss: 3.6709e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 425/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5053e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 425: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 23s 274ms/step - loss: 4.5053e-04 - mean_absolute_error: 0.0172 - val_loss: 4.0515e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 426/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2436e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 426: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 58s 684ms/step - loss: 4.2436e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7165e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 427/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0352e-04 - mean_absolute_error: 0.0166 \n",
      "Epoch 427: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 1456s 17s/step - loss: 4.0352e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6638e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 428/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1590e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 428: val_loss improved from 0.00036 to 0.00036, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 25s 298ms/step - loss: 4.1590e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6468e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 429/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3068e-04 - mean_absolute_error: 0.0170 \n",
      "Epoch 429: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 997s 12s/step - loss: 4.3068e-04 - mean_absolute_error: 0.0170 - val_loss: 4.1718e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 430/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0014e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 430: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 25s 292ms/step - loss: 4.0014e-04 - mean_absolute_error: 0.0165 - val_loss: 3.8544e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 431/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1401e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 431: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 682s 8s/step - loss: 4.1401e-04 - mean_absolute_error: 0.0166 - val_loss: 4.0947e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 432/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2389e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 432: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 28s 327ms/step - loss: 4.2389e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7243e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 433/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1584e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 433: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 27s 321ms/step - loss: 4.1584e-04 - mean_absolute_error: 0.0166 - val_loss: 3.7026e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 434/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1478e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 434: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 27s 323ms/step - loss: 4.1478e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6585e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 435/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.8096e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 435: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 28s 326ms/step - loss: 3.8096e-04 - mean_absolute_error: 0.0163 - val_loss: 3.7334e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 436/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0345e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 436: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 28s 326ms/step - loss: 4.0345e-04 - mean_absolute_error: 0.0164 - val_loss: 4.7054e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 437/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5039e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 437: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 27s 323ms/step - loss: 4.5039e-04 - mean_absolute_error: 0.0173 - val_loss: 3.7425e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 438/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1911e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 438: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 28s 327ms/step - loss: 4.1911e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7644e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 439/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1939e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 439: val_loss improved from 0.00036 to 0.00036, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 27s 322ms/step - loss: 4.1939e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6294e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 440/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0694e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 440: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 28s 327ms/step - loss: 4.0694e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7332e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 441/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3857e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 441: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 28s 334ms/step - loss: 4.3857e-04 - mean_absolute_error: 0.0171 - val_loss: 4.4549e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 442/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9980e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 442: val_loss improved from 0.00036 to 0.00036, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 27s 323ms/step - loss: 3.9980e-04 - mean_absolute_error: 0.0165 - val_loss: 3.5870e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 443/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2080e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 443: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 26s 305ms/step - loss: 4.2080e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6897e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 444/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0815e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 444: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 25s 292ms/step - loss: 4.0815e-04 - mean_absolute_error: 0.0165 - val_loss: 3.7265e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 445/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0498e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 445: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 25s 299ms/step - loss: 4.0498e-04 - mean_absolute_error: 0.0166 - val_loss: 4.2446e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 446/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9758e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 446: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 25s 292ms/step - loss: 3.9758e-04 - mean_absolute_error: 0.0164 - val_loss: 3.7850e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 447/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1872e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 447: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 25s 293ms/step - loss: 4.1872e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6403e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 448/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2136e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 448: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 23s 271ms/step - loss: 4.2136e-04 - mean_absolute_error: 0.0169 - val_loss: 3.9675e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 449/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2286e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 449: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.2286e-04 - mean_absolute_error: 0.0168 - val_loss: 3.6859e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 450/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0412e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 450: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.0412e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7163e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 451/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1381e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 451: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.1381e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7525e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 452/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0516e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 452: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 24s 278ms/step - loss: 4.0516e-04 - mean_absolute_error: 0.0165 - val_loss: 3.7107e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 453/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9442e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 453: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 24s 280ms/step - loss: 3.9442e-04 - mean_absolute_error: 0.0165 - val_loss: 3.6458e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 454/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0112e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 454: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 23s 271ms/step - loss: 4.0112e-04 - mean_absolute_error: 0.0165 - val_loss: 3.6775e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 455/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0822e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 455: val_loss improved from 0.00036 to 0.00036, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 22s 258ms/step - loss: 4.0822e-04 - mean_absolute_error: 0.0166 - val_loss: 3.5510e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 456/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9574e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 456: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 22s 262ms/step - loss: 3.9574e-04 - mean_absolute_error: 0.0165 - val_loss: 3.5984e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 457/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2143e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 457: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 22s 261ms/step - loss: 4.2143e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8820e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 458/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9516e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 458: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 24s 277ms/step - loss: 3.9516e-04 - mean_absolute_error: 0.0165 - val_loss: 3.7473e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 459/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2354e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 459: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 4.2354e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7172e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 460/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0756e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 460: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 23s 268ms/step - loss: 4.0756e-04 - mean_absolute_error: 0.0165 - val_loss: 4.1216e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 461/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9260e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 461: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 24s 278ms/step - loss: 3.9260e-04 - mean_absolute_error: 0.0164 - val_loss: 3.7816e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 462/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1208e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 462: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 24s 279ms/step - loss: 4.1208e-04 - mean_absolute_error: 0.0165 - val_loss: 3.6807e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 463/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1422e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 463: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 25s 300ms/step - loss: 4.1422e-04 - mean_absolute_error: 0.0165 - val_loss: 4.0365e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 464/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1953e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 464: val_loss improved from 0.00036 to 0.00035, saving model to results/2024-08-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 23s 275ms/step - loss: 4.1953e-04 - mean_absolute_error: 0.0167 - val_loss: 3.5442e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 465/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.8880e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 465: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 25s 292ms/step - loss: 3.8880e-04 - mean_absolute_error: 0.0163 - val_loss: 3.6846e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 466/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0876e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 466: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 23s 266ms/step - loss: 4.0876e-04 - mean_absolute_error: 0.0165 - val_loss: 3.6448e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 467/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1078e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 467: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 23s 270ms/step - loss: 4.1078e-04 - mean_absolute_error: 0.0165 - val_loss: 4.0378e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 468/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1720e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 468: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 23s 274ms/step - loss: 4.1720e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6820e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 469/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0569e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 469: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 23s 274ms/step - loss: 4.0569e-04 - mean_absolute_error: 0.0164 - val_loss: 3.6079e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 470/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.9309e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 470: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 23s 276ms/step - loss: 3.9309e-04 - mean_absolute_error: 0.0163 - val_loss: 3.6523e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 471/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.8985e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 471: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 277ms/step - loss: 3.8985e-04 - mean_absolute_error: 0.0161 - val_loss: 3.6993e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 472/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0939e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 472: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 4.0939e-04 - mean_absolute_error: 0.0166 - val_loss: 4.1632e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 473/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3061e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 473: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 286ms/step - loss: 4.3061e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6318e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 474/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0257e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 474: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 281ms/step - loss: 4.0257e-04 - mean_absolute_error: 0.0166 - val_loss: 3.9780e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 475/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0154e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 475: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 281ms/step - loss: 4.0154e-04 - mean_absolute_error: 0.0165 - val_loss: 3.6956e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 476/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0625e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 476: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 280ms/step - loss: 4.0625e-04 - mean_absolute_error: 0.0165 - val_loss: 3.6661e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 477/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2826e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 477: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 281ms/step - loss: 4.2826e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6796e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 478/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.8878e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 478: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 282ms/step - loss: 3.8878e-04 - mean_absolute_error: 0.0162 - val_loss: 3.7525e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 479/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1145e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 479: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 281ms/step - loss: 4.1145e-04 - mean_absolute_error: 0.0166 - val_loss: 3.9696e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 480/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0455e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 480: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 283ms/step - loss: 4.0455e-04 - mean_absolute_error: 0.0164 - val_loss: 3.8492e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 481/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0696e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 481: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 284ms/step - loss: 4.0696e-04 - mean_absolute_error: 0.0163 - val_loss: 4.1206e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 482/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1859e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 482: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 285ms/step - loss: 4.1859e-04 - mean_absolute_error: 0.0167 - val_loss: 3.7429e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 483/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0345e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 483: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 25s 299ms/step - loss: 4.0345e-04 - mean_absolute_error: 0.0165 - val_loss: 4.2462e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 484/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 8.0142e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 484: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 287ms/step - loss: 8.0142e-04 - mean_absolute_error: 0.0197 - val_loss: 3.8064e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 485/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0719e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 485: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 4.0719e-04 - mean_absolute_error: 0.0167 - val_loss: 4.4869e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 486/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1057e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 486: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 24s 282ms/step - loss: 4.1057e-04 - mean_absolute_error: 0.0166 - val_loss: 3.8376e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 487/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3330e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 487: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 25s 295ms/step - loss: 4.3330e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8399e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 488/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2545e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 488: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 308ms/step - loss: 4.2545e-04 - mean_absolute_error: 0.0168 - val_loss: 3.5956e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 489/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0823e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 489: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 305ms/step - loss: 4.0823e-04 - mean_absolute_error: 0.0165 - val_loss: 3.6508e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 490/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1932e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 490: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 25s 296ms/step - loss: 4.1932e-04 - mean_absolute_error: 0.0169 - val_loss: 4.2582e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 491/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1296e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 491: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 25s 293ms/step - loss: 4.1296e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6859e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 492/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2665e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 492: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 23s 276ms/step - loss: 4.2665e-04 - mean_absolute_error: 0.0169 - val_loss: 4.3175e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 493/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3266e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 493: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 25s 296ms/step - loss: 4.3266e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8961e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 494/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0368e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 494: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 4.0368e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6024e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 495/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1570e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 495: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 4.1570e-04 - mean_absolute_error: 0.0167 - val_loss: 3.8589e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 496/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1582e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 496: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 303ms/step - loss: 4.1582e-04 - mean_absolute_error: 0.0165 - val_loss: 3.7006e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 497/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1499e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 497: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 4.1499e-04 - mean_absolute_error: 0.0169 - val_loss: 3.5781e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 498/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1144e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 498: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 4.1144e-04 - mean_absolute_error: 0.0166 - val_loss: 3.5781e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 499/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.8870e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 499: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 305ms/step - loss: 3.8870e-04 - mean_absolute_error: 0.0163 - val_loss: 3.6133e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 500/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 3.8950e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 500: val_loss did not improve from 0.00035\n",
      "85/85 [==============================] - 26s 301ms/step - loss: 3.8950e-04 - mean_absolute_error: 0.0163 - val_loss: 3.8676e-04 - val_mean_absolute_error: 0.0125\n"
     ]
    }
   ],
   "source": [
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 19:39:59.029140: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 19:39:59.030389: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 19:39:59.031056: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-14 19:39:59.100385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-14 19:39:59.100951: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-14 19:39:59.101482: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 3s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 178.51$\n",
      "huber_loss loss: 0.00035441547515802085\n",
      "Mean Absolute Error: 2.5789963850588107\n",
      "Accuracy score: 0.5879323031640913\n",
      "Total buy profit: 692.0112848877907\n",
      "Total sell profit: 161.9179294705391\n",
      "Total profit: 853.9292143583298\n",
      "Profit per trade: 0.628351151109882\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-07-29</th>\n",
       "      <td>0.118229</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.123958</td>\n",
       "      <td>0.123958</td>\n",
       "      <td>96288000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.835007</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-07</th>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.113021</td>\n",
       "      <td>0.106250</td>\n",
       "      <td>0.108854</td>\n",
       "      <td>0.108854</td>\n",
       "      <td>40680000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.808498</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-21</th>\n",
       "      <td>0.106771</td>\n",
       "      <td>0.108594</td>\n",
       "      <td>0.103646</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>12480000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.835847</td>\n",
       "      <td>0.184375</td>\n",
       "      <td>0.078646</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-03</th>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>0.115625</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>39288000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.829578</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.102083</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-05</th>\n",
       "      <td>0.129167</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.122917</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>38160000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.786809</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-08</th>\n",
       "      <td>0.126563</td>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>112968000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.806144</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-10</th>\n",
       "      <td>0.165625</td>\n",
       "      <td>0.166406</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>77328000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.695449</td>\n",
       "      <td>0.201042</td>\n",
       "      <td>0.035938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-15</th>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.183854</td>\n",
       "      <td>0.152604</td>\n",
       "      <td>0.154688</td>\n",
       "      <td>0.154688</td>\n",
       "      <td>111672000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.782147</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.051562</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-16</th>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.155990</td>\n",
       "      <td>0.167708</td>\n",
       "      <td>0.167708</td>\n",
       "      <td>128640000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.702873</td>\n",
       "      <td>0.202865</td>\n",
       "      <td>0.035157</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-17</th>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.170313</td>\n",
       "      <td>0.170313</td>\n",
       "      <td>52152000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.776489</td>\n",
       "      <td>0.200260</td>\n",
       "      <td>0.029947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-29</th>\n",
       "      <td>0.207292</td>\n",
       "      <td>0.209375</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>47424000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.949056</td>\n",
       "      <td>0.191146</td>\n",
       "      <td>-0.010937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-06</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.197135</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>40560000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.986114</td>\n",
       "      <td>0.213542</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-17</th>\n",
       "      <td>0.180729</td>\n",
       "      <td>0.182813</td>\n",
       "      <td>0.176042</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>50688000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.819250</td>\n",
       "      <td>0.223958</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-21</th>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.221875</td>\n",
       "      <td>0.192708</td>\n",
       "      <td>0.221354</td>\n",
       "      <td>0.221354</td>\n",
       "      <td>241920000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.751378</td>\n",
       "      <td>0.197396</td>\n",
       "      <td>-0.023958</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-11-05</th>\n",
       "      <td>0.248958</td>\n",
       "      <td>0.255990</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>61872000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.922571</td>\n",
       "      <td>0.213021</td>\n",
       "      <td>-0.030729</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-11-17</th>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.227083</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>147888000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.848469</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-03</th>\n",
       "      <td>0.209896</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.209375</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>25800000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.988590</td>\n",
       "      <td>0.230208</td>\n",
       "      <td>0.011979</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-12</th>\n",
       "      <td>0.232292</td>\n",
       "      <td>0.240104</td>\n",
       "      <td>0.220313</td>\n",
       "      <td>0.227083</td>\n",
       "      <td>0.227083</td>\n",
       "      <td>53448000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.825375</td>\n",
       "      <td>0.241927</td>\n",
       "      <td>0.014844</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-16</th>\n",
       "      <td>0.232292</td>\n",
       "      <td>0.232292</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>33744000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.934678</td>\n",
       "      <td>0.230729</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-18</th>\n",
       "      <td>0.217708</td>\n",
       "      <td>0.221354</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.214583</td>\n",
       "      <td>0.214583</td>\n",
       "      <td>64080000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.796256</td>\n",
       "      <td>0.215104</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close  adjclose     volume  \\\n",
       "1997-07-29  0.118229  0.125000  0.116667  0.123958  0.123958   96288000   \n",
       "1997-08-07  0.112500  0.113021  0.106250  0.108854  0.108854   40680000   \n",
       "1997-08-21  0.106771  0.108594  0.103646  0.105729  0.105729   12480000   \n",
       "1997-09-03  0.117188  0.120833  0.115625  0.116667  0.116667   39288000   \n",
       "1997-09-05  0.129167  0.133333  0.122917  0.125000  0.125000   38160000   \n",
       "1997-09-08  0.126563  0.151042  0.125000  0.150000  0.150000  112968000   \n",
       "1997-09-10  0.165625  0.166406  0.156250  0.165104  0.165104   77328000   \n",
       "1997-09-15  0.183333  0.183854  0.152604  0.154688  0.154688  111672000   \n",
       "1997-09-16  0.156250  0.177083  0.155990  0.167708  0.167708  128640000   \n",
       "1997-09-17  0.172917  0.175000  0.166667  0.170313  0.170313   52152000   \n",
       "1997-09-29  0.207292  0.209375  0.197917  0.202083  0.202083   47424000   \n",
       "1997-10-06  0.200000  0.206250  0.197135  0.206250  0.206250   40560000   \n",
       "1997-10-17  0.180729  0.182813  0.176042  0.181250  0.181250   50688000   \n",
       "1997-10-21  0.197917  0.221875  0.192708  0.221354  0.221354  241920000   \n",
       "1997-11-05  0.248958  0.255990  0.243750  0.243750  0.243750   61872000   \n",
       "1997-11-17  0.211458  0.227083  0.210938  0.218750  0.218750  147888000   \n",
       "1997-12-03  0.209896  0.218750  0.209375  0.218229  0.218229   25800000   \n",
       "1997-12-12  0.232292  0.240104  0.220313  0.227083  0.227083   53448000   \n",
       "1997-12-16  0.232292  0.232292  0.222917  0.222917  0.222917   33744000   \n",
       "1997-12-18  0.217708  0.221354  0.211458  0.214583  0.214583   64080000   \n",
       "\n",
       "           ticker  adjclose_15  true_adjclose_15  buy_profit  sell_profit  \n",
       "1997-07-29   AMZN     0.835007          0.108333   -0.015625          0.0  \n",
       "1997-08-07   AMZN     0.808498          0.118750    0.009896          0.0  \n",
       "1997-08-21   AMZN     0.835847          0.184375    0.078646          0.0  \n",
       "1997-09-03   AMZN     0.829578          0.218750    0.102083          0.0  \n",
       "1997-09-05   AMZN     0.786809          0.208333    0.083333          0.0  \n",
       "1997-09-08   AMZN     0.806144          0.202083    0.052083          0.0  \n",
       "1997-09-10   AMZN     0.695449          0.201042    0.035938          0.0  \n",
       "1997-09-15   AMZN     0.782147          0.206250    0.051562          0.0  \n",
       "1997-09-16   AMZN     0.702873          0.202865    0.035157          0.0  \n",
       "1997-09-17   AMZN     0.776489          0.200260    0.029947          0.0  \n",
       "1997-09-29   AMZN     0.949056          0.191146   -0.010937          0.0  \n",
       "1997-10-06   AMZN     0.986114          0.213542    0.007292          0.0  \n",
       "1997-10-17   AMZN     0.819250          0.223958    0.042708          0.0  \n",
       "1997-10-21   AMZN     0.751378          0.197396   -0.023958          0.0  \n",
       "1997-11-05   AMZN     0.922571          0.213021   -0.030729          0.0  \n",
       "1997-11-17   AMZN     0.848469          0.234375    0.015625          0.0  \n",
       "1997-12-03   AMZN     0.988590          0.230208    0.011979          0.0  \n",
       "1997-12-12   AMZN     0.825375          0.241927    0.014844          0.0  \n",
       "1997-12-16   AMZN     0.934678          0.230729    0.007812          0.0  \n",
       "1997-12-18   AMZN     0.796256          0.215104    0.000521          0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-15</th>\n",
       "      <td>170.580002</td>\n",
       "      <td>171.169998</td>\n",
       "      <td>167.589996</td>\n",
       "      <td>169.800003</td>\n",
       "      <td>169.800003</td>\n",
       "      <td>49855200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>166.563522</td>\n",
       "      <td>175.350006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.550003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-20</th>\n",
       "      <td>167.830002</td>\n",
       "      <td>168.710007</td>\n",
       "      <td>165.740005</td>\n",
       "      <td>167.080002</td>\n",
       "      <td>167.080002</td>\n",
       "      <td>41980300</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>165.323730</td>\n",
       "      <td>175.389999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.309998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-23</th>\n",
       "      <td>174.279999</td>\n",
       "      <td>175.750000</td>\n",
       "      <td>173.699997</td>\n",
       "      <td>174.990005</td>\n",
       "      <td>174.990005</td>\n",
       "      <td>59715200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>169.987396</td>\n",
       "      <td>174.419998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-27</th>\n",
       "      <td>174.080002</td>\n",
       "      <td>174.619995</td>\n",
       "      <td>172.860001</td>\n",
       "      <td>173.539993</td>\n",
       "      <td>173.539993</td>\n",
       "      <td>31141700</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>171.236755</td>\n",
       "      <td>175.899994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-04</th>\n",
       "      <td>177.529999</td>\n",
       "      <td>180.139999</td>\n",
       "      <td>177.490005</td>\n",
       "      <td>177.580002</td>\n",
       "      <td>177.580002</td>\n",
       "      <td>37381500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>173.727768</td>\n",
       "      <td>179.710007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.130005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-09</th>\n",
       "      <td>187.240005</td>\n",
       "      <td>187.339996</td>\n",
       "      <td>184.199997</td>\n",
       "      <td>185.669998</td>\n",
       "      <td>185.669998</td>\n",
       "      <td>36546900</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>180.601883</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-11</th>\n",
       "      <td>186.740005</td>\n",
       "      <td>189.770004</td>\n",
       "      <td>185.509995</td>\n",
       "      <td>189.050003</td>\n",
       "      <td>189.050003</td>\n",
       "      <td>40020700</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>181.153458</td>\n",
       "      <td>184.720001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.330002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18</th>\n",
       "      <td>181.470001</td>\n",
       "      <td>182.389999</td>\n",
       "      <td>178.649994</td>\n",
       "      <td>179.220001</td>\n",
       "      <td>179.220001</td>\n",
       "      <td>30723800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>178.144608</td>\n",
       "      <td>189.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.279999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-22</th>\n",
       "      <td>176.940002</td>\n",
       "      <td>178.869995</td>\n",
       "      <td>174.559998</td>\n",
       "      <td>177.229996</td>\n",
       "      <td>177.229996</td>\n",
       "      <td>37924900</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>175.415131</td>\n",
       "      <td>186.570007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.340012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-25</th>\n",
       "      <td>169.679993</td>\n",
       "      <td>173.919998</td>\n",
       "      <td>166.320007</td>\n",
       "      <td>173.669998</td>\n",
       "      <td>173.669998</td>\n",
       "      <td>49249400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>174.351776</td>\n",
       "      <td>183.630005</td>\n",
       "      <td>9.960007</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-26</th>\n",
       "      <td>177.800003</td>\n",
       "      <td>180.820007</td>\n",
       "      <td>176.130005</td>\n",
       "      <td>179.619995</td>\n",
       "      <td>179.619995</td>\n",
       "      <td>43919800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>176.485092</td>\n",
       "      <td>184.699997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.080002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-08</th>\n",
       "      <td>187.440002</td>\n",
       "      <td>188.429993</td>\n",
       "      <td>186.389999</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>26136400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>184.159149</td>\n",
       "      <td>179.320007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.679993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-10</th>\n",
       "      <td>189.160004</td>\n",
       "      <td>189.889999</td>\n",
       "      <td>186.929993</td>\n",
       "      <td>187.479996</td>\n",
       "      <td>187.479996</td>\n",
       "      <td>34141800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>183.736969</td>\n",
       "      <td>178.339996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-24</th>\n",
       "      <td>181.649994</td>\n",
       "      <td>182.440002</td>\n",
       "      <td>180.300003</td>\n",
       "      <td>180.750000</td>\n",
       "      <td>180.750000</td>\n",
       "      <td>27434100</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>180.186783</td>\n",
       "      <td>184.059998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.309998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-03</th>\n",
       "      <td>177.699997</td>\n",
       "      <td>178.699997</td>\n",
       "      <td>175.919998</td>\n",
       "      <td>178.339996</td>\n",
       "      <td>178.339996</td>\n",
       "      <td>30786600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>178.778381</td>\n",
       "      <td>186.339996</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-04</th>\n",
       "      <td>177.639999</td>\n",
       "      <td>179.820007</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>179.339996</td>\n",
       "      <td>179.339996</td>\n",
       "      <td>27198400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>179.416168</td>\n",
       "      <td>193.610001</td>\n",
       "      <td>14.270004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-07</th>\n",
       "      <td>184.899994</td>\n",
       "      <td>186.289993</td>\n",
       "      <td>183.360001</td>\n",
       "      <td>184.300003</td>\n",
       "      <td>184.300003</td>\n",
       "      <td>28021500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>183.173157</td>\n",
       "      <td>197.199997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-27</th>\n",
       "      <td>195.009995</td>\n",
       "      <td>199.839996</td>\n",
       "      <td>194.199997</td>\n",
       "      <td>197.850006</td>\n",
       "      <td>197.850006</td>\n",
       "      <td>74397500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>186.078506</td>\n",
       "      <td>183.130005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.720001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-02</th>\n",
       "      <td>197.279999</td>\n",
       "      <td>200.429993</td>\n",
       "      <td>195.929993</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>45600000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>187.367325</td>\n",
       "      <td>180.830002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.169998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-16</th>\n",
       "      <td>195.589996</td>\n",
       "      <td>196.619995</td>\n",
       "      <td>192.240005</td>\n",
       "      <td>193.020004</td>\n",
       "      <td>193.020004</td>\n",
       "      <td>33994700</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>183.613358</td>\n",
       "      <td>161.929993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.090012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "2024-02-15  170.580002  171.169998  167.589996  169.800003  169.800003   \n",
       "2024-02-20  167.830002  168.710007  165.740005  167.080002  167.080002   \n",
       "2024-02-23  174.279999  175.750000  173.699997  174.990005  174.990005   \n",
       "2024-02-27  174.080002  174.619995  172.860001  173.539993  173.539993   \n",
       "2024-03-04  177.529999  180.139999  177.490005  177.580002  177.580002   \n",
       "2024-04-09  187.240005  187.339996  184.199997  185.669998  185.669998   \n",
       "2024-04-11  186.740005  189.770004  185.509995  189.050003  189.050003   \n",
       "2024-04-18  181.470001  182.389999  178.649994  179.220001  179.220001   \n",
       "2024-04-22  176.940002  178.869995  174.559998  177.229996  177.229996   \n",
       "2024-04-25  169.679993  173.919998  166.320007  173.669998  173.669998   \n",
       "2024-04-26  177.800003  180.820007  176.130005  179.619995  179.619995   \n",
       "2024-05-08  187.440002  188.429993  186.389999  188.000000  188.000000   \n",
       "2024-05-10  189.160004  189.889999  186.929993  187.479996  187.479996   \n",
       "2024-05-24  181.649994  182.440002  180.300003  180.750000  180.750000   \n",
       "2024-06-03  177.699997  178.699997  175.919998  178.339996  178.339996   \n",
       "2024-06-04  177.639999  179.820007  176.440002  179.339996  179.339996   \n",
       "2024-06-07  184.899994  186.289993  183.360001  184.300003  184.300003   \n",
       "2024-06-27  195.009995  199.839996  194.199997  197.850006  197.850006   \n",
       "2024-07-02  197.279999  200.429993  195.929993  200.000000  200.000000   \n",
       "2024-07-16  195.589996  196.619995  192.240005  193.020004  193.020004   \n",
       "\n",
       "              volume ticker  adjclose_15  true_adjclose_15  buy_profit  \\\n",
       "2024-02-15  49855200   AMZN   166.563522        175.350006    0.000000   \n",
       "2024-02-20  41980300   AMZN   165.323730        175.389999    0.000000   \n",
       "2024-02-23  59715200   AMZN   169.987396        174.419998    0.000000   \n",
       "2024-02-27  31141700   AMZN   171.236755        175.899994    0.000000   \n",
       "2024-03-04  37381500   AMZN   173.727768        179.710007    0.000000   \n",
       "2024-04-09  36546900   AMZN   180.601883        175.000000    0.000000   \n",
       "2024-04-11  40020700   AMZN   181.153458        184.720001    0.000000   \n",
       "2024-04-18  30723800   AMZN   178.144608        189.500000    0.000000   \n",
       "2024-04-22  37924900   AMZN   175.415131        186.570007    0.000000   \n",
       "2024-04-25  49249400   AMZN   174.351776        183.630005    9.960007   \n",
       "2024-04-26  43919800   AMZN   176.485092        184.699997    0.000000   \n",
       "2024-05-08  26136400   AMZN   184.159149        179.320007    0.000000   \n",
       "2024-05-10  34141800   AMZN   183.736969        178.339996    0.000000   \n",
       "2024-05-24  27434100   AMZN   180.186783        184.059998    0.000000   \n",
       "2024-06-03  30786600   AMZN   178.778381        186.339996    8.000000   \n",
       "2024-06-04  27198400   AMZN   179.416168        193.610001   14.270004   \n",
       "2024-06-07  28021500   AMZN   183.173157        197.199997    0.000000   \n",
       "2024-06-27  74397500   AMZN   186.078506        183.130005    0.000000   \n",
       "2024-07-02  45600000   AMZN   187.367325        180.830002    0.000000   \n",
       "2024-07-16  33994700   AMZN   183.613358        161.929993    0.000000   \n",
       "\n",
       "            sell_profit  \n",
       "2024-02-15    -5.550003  \n",
       "2024-02-20    -8.309998  \n",
       "2024-02-23     0.570007  \n",
       "2024-02-27    -2.360001  \n",
       "2024-03-04    -2.130005  \n",
       "2024-04-09    10.669998  \n",
       "2024-04-11     4.330002  \n",
       "2024-04-18   -10.279999  \n",
       "2024-04-22    -9.340012  \n",
       "2024-04-25     0.000000  \n",
       "2024-04-26    -5.080002  \n",
       "2024-05-08     8.679993  \n",
       "2024-05-10     9.139999  \n",
       "2024-05-24    -3.309998  \n",
       "2024-06-03     0.000000  \n",
       "2024-06-04     0.000000  \n",
       "2024-06-07   -12.899994  \n",
       "2024-06-27    14.720001  \n",
       "2024-07-02    19.169998  \n",
       "2024-07-16    31.090012  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
